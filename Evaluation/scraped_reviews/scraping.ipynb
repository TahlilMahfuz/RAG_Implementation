{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q79reDvPwgPn",
        "outputId": "d9feeadf-3ea6-4c8b-9913-e89121e0e0e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google-play-scraper\n",
            "  Downloading google_play_scraper-1.2.7-py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_play_scraper-1.2.7-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: google-play-scraper\n",
            "Successfully installed google-play-scraper-1.2.7\n"
          ]
        }
      ],
      "source": [
        "# pip install google-play-scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dZqnd1jwvsm",
        "outputId": "d6abdcc4-9c21-48b2-c97c-f80a6bbbdb46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting scraping...\n"
          ]
        }
      ],
      "source": [
        "from google_play_scraper import reviews, Sort\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# App details\n",
        "app_id = \"com.bitsmedia.android.muslimpro\"\n",
        "app_name = \"মুসলিম প্রো: কুরআন আযান নামাজ\"\n",
        "\n",
        "# Configuration\n",
        "batch_size = 10000  # Number of reviews per batch\n",
        "total_reviews = 40000  # Target total to try\n",
        "all_reviews = []\n",
        "continuation_token = None\n",
        "\n",
        "print(\"Starting scraping...\")\n",
        "\n",
        "while len(all_reviews) < total_reviews:\n",
        "    result, continuation_token = reviews(\n",
        "        app_id,\n",
        "        lang='en',  # Adjust if needed\n",
        "        country='us',\n",
        "        sort=Sort.NEWEST,\n",
        "        count=batch_size,\n",
        "        continuation_token=continuation_token\n",
        "    )\n",
        "\n",
        "    # Filter this batch to include only reviews with replies\n",
        "    filtered_batch = [\n",
        "        {\n",
        "            \"content\": r[\"content\"],\n",
        "            \"replyContent\": r[\"replyContent\"],\n",
        "            \"score\": r[\"score\"],\n",
        "            \"app_name\": app_name\n",
        "        }\n",
        "        for r in result if r.get(\"replyContent\")\n",
        "    ]\n",
        "\n",
        "    all_reviews.extend(filtered_batch)\n",
        "    print(f\"Collected {len(all_reviews)} reviews with replies...\")\n",
        "\n",
        "    # Break if no more reviews to fetch\n",
        "    if continuation_token is None:\n",
        "        break\n",
        "\n",
        "    # Be nice to Google servers (sleep optional)\n",
        "    time.sleep(1)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(all_reviews)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"filtered_reviews.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"Scraping completed! {len(df)} reviews with replies saved to 'filtered_reviews.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google_play_scraper import reviews, Sort\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# App details\n",
        "app_id = \"com.greentech.quran\"\n",
        "app_name = \"Al Quran (Tafsir & by Word)\"\n",
        "\n",
        "# Configuration\n",
        "batch_size = 10000  # Number of reviews per batch\n",
        "total_reviews = 40000  # Target total to try\n",
        "all_reviews = []\n",
        "continuation_token = None\n",
        "\n",
        "print(\"Starting scraping...\")\n",
        "\n",
        "while len(all_reviews) < total_reviews:\n",
        "    result, continuation_token = reviews(\n",
        "        app_id,\n",
        "        lang='en',  # Adjust if needed\n",
        "        country='us',\n",
        "        sort=Sort.NEWEST,\n",
        "        count=batch_size,\n",
        "        continuation_token=continuation_token\n",
        "    )\n",
        "\n",
        "    # Filter this batch to include only reviews with replies\n",
        "    filtered_batch = [\n",
        "        {\n",
        "            \"review\": r[\"content\"],\n",
        "            \"reply\": r[\"replyContent\"],\n",
        "            \"score\": r[\"score\"],\n",
        "            \"app_name\": app_name\n",
        "        }\n",
        "        for r in result if r.get(\"replyContent\") and r['score'] <=3\n",
        "    ]\n",
        "\n",
        "    all_reviews.extend(filtered_batch)\n",
        "    print(f\"Collected {len(all_reviews)} reviews with replies...\")\n",
        "\n",
        "    # Break if no more reviews to fetch\n",
        "    if continuation_token is None:\n",
        "        break\n",
        "\n",
        "    # Be nice to Google servers (sleep optional)\n",
        "    time.sleep(1)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(all_reviews)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"filtered_reviews.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"Scraping completed! {len(df)} reviews with replies saved to 'filtered_reviews.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tahlilmahfuz/RAG_Implementation/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-08-23 03:58:35] (2.4.2) PyABSA(2.4.2): If your code crashes on Colab, please use the GPU runtime. Then run \"pip install pyabsa[dev] -U\" and restart the kernel.\n",
            "Or if it does not work, you can use v1.x versions, e.g., pip install pyabsa<2.0 -U\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "WARNING: When you fails to load a checkpoint, e.g., Unexpected key(s),\n",
            "Try to downgrade transformers<=4.29.0.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[2025-08-23 03:58:35] (2.4.2) ********** Available ATEPC model checkpoints for Version:2.4.2 (this version) **********\n",
            "[2025-08-23 03:58:35] (2.4.2) ********** Available ATEPC model checkpoints for Version:2.4.2 (this version) **********\n",
            "[2025-08-23 03:58:35] (2.4.2) Downloading checkpoint:english \n",
            "[2025-08-23 03:58:35] (2.4.2) Notice: The pretrained model are used for testing, it is recommended to train the model on your own custom datasets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading checkpoint: 579MB [02:05,  4.62MB/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Find zipped checkpoint: ./checkpoints/ATEPC_ENGLISH_CHECKPOINT/fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43.zip, unzipping\n",
            "Done.\n",
            "[2025-08-23 04:00:44] (2.4.2) If the auto-downloading failed, please download it via browser: https://huggingface.co/spaces/yangheng/PyABSA/resolve/main/checkpoints/English/ATEPC/fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43.zip \n",
            "[2025-08-23 04:00:44] (2.4.2) Load aspect extractor from checkpoints/ATEPC_ENGLISH_CHECKPOINT/fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\n",
            "[2025-08-23 04:00:44] (2.4.2) config: checkpoints/ATEPC_ENGLISH_CHECKPOINT/fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43/fast_lcf_atepc.config\n",
            "[2025-08-23 04:00:44] (2.4.2) state_dict: checkpoints/ATEPC_ENGLISH_CHECKPOINT/fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43/fast_lcf_atepc.state_dict\n",
            "[2025-08-23 04:00:44] (2.4.2) model: None\n",
            "[2025-08-23 04:00:44] (2.4.2) tokenizer: checkpoints/ATEPC_ENGLISH_CHECKPOINT/fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43/fast_lcf_atepc.tokenizer\n",
            "[2025-08-23 04:00:44] (2.4.2) Set Model Device: cuda:0\n",
            "[2025-08-23 04:00:44] (2.4.2) Device Name: NVIDIA GeForce RTX 4070 Ti SUPER\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tahlilmahfuz/RAG_Implementation/venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from pyabsa import AspectTermExtraction as ATEPC\n",
        "from typing import Generator\n",
        "\n",
        "aspect_extractor = ATEPC.Predictor(\"english\", device='cuda')\n",
        "\n",
        "def extract_aspects_sentiments(text: str) -> list[tuple[str, str]]:\n",
        "    \"\"\"Extract aspects and sentiments from the given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to analyze.\n",
        "\n",
        "    Yields:\n",
        "        list[tuple[str, str]]: A list of tuple containing the aspect and its corresponding sentiment.\n",
        "\n",
        "    Example:\n",
        "        >>> sample_text = \"The food was great but the service was terrible.\"\n",
        "        >>> for aspect, sentiment in extract_aspects_sentiments(sample_text):\n",
        "        ...     print(f\"Aspect: {aspect}, Sentiment: {sentiment}\")\n",
        "        Aspect: food, Sentiment: positive\n",
        "        Aspect: service, Sentiment: negative\n",
        "    \"\"\"\n",
        "    result = aspect_extractor.predict(text)\n",
        "    results = []\n",
        "    for aspect, sentiment in zip(result['aspect'], result['sentiment']):\n",
        "        results.append((aspect, sentiment))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-08-23 04:02:10] (2.4.2) Can not load en_core_web_sm from spacy, try to download it in order to parse syntax tree: \n",
            "python -m spacy download en_core_web_sm\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tahlilmahfuz/RAG_Implementation/venv/lib/python3.12/site-packages/pyabsa/tasks/AspectTermExtraction/prediction/aspect_extractor.py:593: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  lcf_cdm_vec = torch.tensor(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-08-23 04:03:32] (2.4.2) The results of aspect term extraction have been saved in /home/tahlilmahfuz/RAG_Implementation/Evaluation/Aspect Term Extraction and Polarity Classification.FAST_LCF_ATEPC.result.json\n",
            "[2025-08-23 04:03:32] (2.4.2) Example 0: already turned on and set all things that had been told to do so . no clean <apps:Negative Confidence:0.9988> or the same function to it were installed before it caused the problems .\n"
          ]
        }
      ],
      "source": [
        "extracted_aspects = extract_aspects_sentiments(\"already turned on and set all things that had been told to do so. no clean apps or the same function to it were installed before it caused the problems.\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('apps', 'Negative')]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extracted_aspects"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
