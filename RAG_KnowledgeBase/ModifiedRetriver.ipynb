{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fbc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tahlilmahfuz/RAG_Implementation/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.chains import create_extraction_chain\n",
    "from typing import Optional, List\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from agentic_chunker import AgenticChunker\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from langsmith import Client\n",
    "import json\n",
    "# from agentic_chunker import AgenticChunker\n",
    "from langchain.docstore.document import Document\n",
    "from dotenv import load_dotenv\n",
    "from rich import print\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community import embeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc51cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Cerebras client\n",
    "cerebras_api_key = os.getenv(\"CEREBRAS_API_KEY\")\n",
    "if not cerebras_api_key:\n",
    "    raise ValueError(\"CEREBRAS_API_KEY not found in environment variables\")\n",
    "\n",
    "client = Cerebras(api_key=cerebras_api_key)\n",
    "model = \"llama-4-scout-17b-16e-instruct\"\n",
    "\n",
    "# Function to invoke Cerebras API\n",
    "def cerebras_invoke(prompt: str) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=model,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Cerebras API invocation failed: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76fc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the proposal-indexing prompt from the hub\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "obj = Client(api_key=LANGSMITH_API_KEY).pull_prompt(\"wfh/proposal-indexing\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35cdd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentences(BaseModel):\n",
    "    sentences: List[str]\n",
    "\n",
    "def get_propositions(text, prompt):\n",
    "    formatted_prompt = prompt.format(input=text) + \"\\n\\nOnly provide the list of propositions as output. Do not include any explanations, formatting, or additional text.\"\n",
    "    # print(f\"Formatted Prompt: {formatted_prompt}\")\n",
    "    response = cerebras_invoke(formatted_prompt)\n",
    "    # print(f\"Response: {response}\")\n",
    "    propositions = response.split('\\n')\n",
    "    return {\"proposition\": [Sentences(sentences=propositions)]}, response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa1f740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "with open(\"islamic_etiquette_knowledge_base.json\", \"r\") as f1, open(\"Quran_app_Documentation.json\", \"r\") as f2:\n",
    "    etiquette_data = json.load(f1)\n",
    "    quran_app_data = json.load(f2)\n",
    "\n",
    "# Use only Quran app data as per the query\n",
    "combined_documents = quran_app_data[:2]\n",
    "\n",
    "# List to hold all proposition arrays with metadata\n",
    "proposition_arrays = []\n",
    "\n",
    "# Process each JSON object\n",
    "for json_obj in combined_documents:\n",
    "    text = json_obj['text']\n",
    "    propositions, response = get_propositions(text, obj)\n",
    "    \n",
    "    # Create an array entry for this document's propositions\n",
    "    document_propositions = {\n",
    "        'metadata': {\n",
    "            'url': json_obj['url'],\n",
    "            'title': json_obj['title']\n",
    "        },\n",
    "        'propositions': [\n",
    "            prop for prop in propositions['proposition'][0].sentences if prop.strip()\n",
    "        ]\n",
    "    }\n",
    "    proposition_arrays.append(document_propositions)\n",
    "\n",
    "# If you need a flat list of all propositions with their metadata:\n",
    "flat_propositions_with_metadata = []\n",
    "for doc in proposition_arrays:\n",
    "    for prop in doc['propositions']:\n",
    "        flat_propositions_with_metadata.append({\n",
    "            'proposition': prop,\n",
    "            'metadata': doc['metadata']\n",
    "        })\n",
    "\n",
    "# And if you just need a simple list of all propositions:\n",
    "propositions_list = [prop for doc in proposition_arrays for prop in doc['propositions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c8aed14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m65\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(propositions_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7ddcc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "from typing import Optional\n",
    "from rich import print\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "\n",
    "load_dotenv()\n",
    "class AgenticChunker:\n",
    "    def __init__(self, cerebras_api_key: Optional[str] = None):\n",
    "        self.chunks = {}\n",
    "        self.id_truncate_limit = 5\n",
    "        self.generate_new_metadata_ind = True\n",
    "        self.print_logging = True\n",
    "\n",
    "        if cerebras_api_key is None:\n",
    "            cerebras_api_key = os.getenv(\"CEREBRAS_API_KEY\")\n",
    "        if cerebras_api_key is None:\n",
    "            raise ValueError(\"CEREBRAS_API_KEY not provided or found in environment variables\")\n",
    "\n",
    "        self.client = Cerebras(api_key=cerebras_api_key)\n",
    "        self.model = \"llama-4-scout-17b-16e-instruct\"\n",
    "\n",
    "    def _llm_invoke(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=self.model,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] LLM invocation failed: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def add_propositions(self, propositions: list):\n",
    "        for proposition in propositions:\n",
    "            self.add_proposition(proposition)\n",
    "\n",
    "    def add_proposition(self, proposition: str):\n",
    "        if self.print_logging:\n",
    "            print(f\"\\nAdding: '{proposition}'\")\n",
    "        if len(self.chunks) == 0:\n",
    "            if self.print_logging:\n",
    "                print(\"No chunks, creating a new one\")\n",
    "            self._create_new_chunk(proposition)\n",
    "            return\n",
    "\n",
    "        chunk_id = self._find_relevant_chunk(proposition)\n",
    "        if chunk_id:\n",
    "            if self.print_logging:\n",
    "                print(f\"Chunk Found ({self.chunks[chunk_id]['chunk_id']}), adding to: {self.chunks[chunk_id]['title']}\")\n",
    "            self.add_proposition_to_chunk(chunk_id, proposition)\n",
    "        else:\n",
    "            if self.print_logging:\n",
    "                print(\"No chunks found\")\n",
    "            self._create_new_chunk(proposition)\n",
    "\n",
    "    def add_proposition_to_chunk(self, chunk_id: str, proposition: str):\n",
    "        self.chunks[chunk_id]['propositions'].append(proposition)\n",
    "        if self.generate_new_metadata_ind:\n",
    "            self.chunks[chunk_id]['summary'] = self._update_chunk_summary(self.chunks[chunk_id])\n",
    "            self.chunks[chunk_id]['title'] = self._update_chunk_title(self.chunks[chunk_id])\n",
    "\n",
    "    def _update_chunk_summary(self, chunk: dict) -> str:\n",
    "        prompt = (\n",
    "            \"You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic.\\n\"\n",
    "            \"A new proposition was just added to one of your chunks. Generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\\n\"\n",
    "            \"A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\\n\"\n",
    "            \"Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food. Or month, generalize it to 'date and times'.\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"Input: Proposition: Greg likes to eat pizza\\n\"\n",
    "            \"Output: This chunk contains information about the types of food Greg likes to eat.\\n\"\n",
    "            \"Only respond with the chunk new summary, nothing else.\\n\"\n",
    "            f\"Chunk's propositions:\\n\" + \"\\n\".join(chunk['propositions']) +\n",
    "            f\"\\n\\nCurrent chunk summary:\\n{chunk['summary']}\"\n",
    "        )\n",
    "        return self._llm_invoke(prompt)\n",
    "\n",
    "    def _update_chunk_title(self, chunk: dict) -> str:\n",
    "        prompt = (\n",
    "            \"You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic.\\n\"\n",
    "            \"A new proposition was just added to one of your chunks, you should generate a very brief updated chunk title which will inform viewers what a chunk group is about.\\n\"\n",
    "            \"A good title will say what the chunk is about.\\n\"\n",
    "            \"You will be given a group of propositions which are in the chunk, chunk summary and the chunk title.\\n\"\n",
    "            \"Your title should anticipate generalization. If you get a proposition about apples, generalize it to food. Or month, generalize it to \\\"date and times\\\".\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"Input: Summary: This chunk is about dates and times that the author talks about\\n\"\n",
    "            \"Output: Date & Times\\n\"\n",
    "            \"Only respond with the new chunk title, nothing else.\\n\"\n",
    "            f\"Chunk's propositions:\\n\" + \"\\n\".join(chunk['propositions']) +\n",
    "            f\"\\n\\nChunk summary:\\n{chunk['summary']}\\n\\nCurrent chunk title:\\n{chunk['title']}\"\n",
    "        )\n",
    "        return self._llm_invoke(prompt)\n",
    "\n",
    "    def _get_new_chunk_summary(self, proposition: str) -> str:\n",
    "        prompt = (\n",
    "            \"You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic.\\n\"\n",
    "            \"You should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\\n\"\n",
    "            \"A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\\n\"\n",
    "            \"You will be given a proposition which will go into a new chunk. This new chunk needs a summary.\\n\"\n",
    "            \"Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food. Or month, generalize it to \\\"date and times\\\".\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"Input: Proposition: Greg likes to eat pizza\\n\"\n",
    "            \"Output: This chunk contains information about the types of food Greg likes to eat.\\n\"\n",
    "            \"Only respond with the new chunk summary, nothing else.\\n\"\n",
    "            f\"Determine the summary of the new chunk that this proposition will go into:\\n{proposition}\"\n",
    "        )\n",
    "        return self._llm_invoke(prompt)\n",
    "\n",
    "    def _get_new_chunk_title(self, summary: str) -> str:\n",
    "        prompt = (\n",
    "            \"You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic.\\n\"\n",
    "            \"You should generate a very brief few word chunk title which will inform viewers what a chunk group is about.\\n\"\n",
    "            \"A good chunk title is brief but encompasses what the chunk is about.\\n\"\n",
    "            \"You will be given a summary of a chunk which needs a title.\\n\"\n",
    "            \"Your titles should anticipate generalization. If you get a proposition about apples, generalize it to food. Or month, generalize it to \\\"date and times\\\".\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"Input: Summary: This chunk is about dates and times that the author talks about\\n\"\n",
    "            \"Output: Date & Times\\n\"\n",
    "            \"Only respond with the new chunk title, nothing else.\\n\"\n",
    "            f\"Determine the title of the chunk that this summary belongs to:\\n{summary}\"\n",
    "        )\n",
    "        return self._llm_invoke(prompt)\n",
    "\n",
    "    def _create_new_chunk(self, proposition: str):\n",
    "        new_chunk_id = str(uuid.uuid4())[:self.id_truncate_limit]\n",
    "        new_chunk_summary = self._get_new_chunk_summary(proposition)\n",
    "        new_chunk_title = self._get_new_chunk_title(new_chunk_summary)\n",
    "        self.chunks[new_chunk_id] = {\n",
    "            'chunk_id': new_chunk_id,\n",
    "            'propositions': [proposition],\n",
    "            'title': new_chunk_title,\n",
    "            'summary': new_chunk_summary,\n",
    "            'chunk_index': len(self.chunks)\n",
    "        }\n",
    "        if self.print_logging:\n",
    "            print(f\"Created new chunk ({new_chunk_id}): {new_chunk_title}\")\n",
    "\n",
    "    def get_chunk_outline(self) -> str:\n",
    "        chunk_outline = \"\"\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            single_chunk_string = f\"\"\"Chunk ({chunk['chunk_id']}): {chunk['title']}\\nSummary: {chunk['summary']}\\n\\n\"\"\"\n",
    "            chunk_outline += single_chunk_string\n",
    "        return chunk_outline\n",
    "\n",
    "    def _find_relevant_chunk(self, proposition: str) -> Optional[str]:\n",
    "        current_chunk_outline = self.get_chunk_outline()\n",
    "        prompt = (\n",
    "            \"Determine whether or not the 'Proposition' should belong to any of the existing chunks.\\n\"\n",
    "            \"A proposition should belong to a chunk if their meaning, direction, or intention are similar.\\n\"\n",
    "            \"The goal is to group similar propositions and chunks.\\n\"\n",
    "            \"If you think a proposition should be joined with a chunk, return the chunk id.\\n\"\n",
    "            \"If you do not think an item should be joined with an existing chunk, just return 'No chunks'.\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"Input:\\n\"\n",
    "            \"    - Proposition: 'Greg really likes hamburgers'\\n\"\n",
    "            \"    - Current Chunks:\\n\"\n",
    "            \"        - Chunk ID: 2n4l3d\\n\"\n",
    "            \"        - Chunk Name: Places in San Francisco\\n\"\n",
    "            \"        - Chunk Summary: Overview of the things to do with San Francisco Places\\n\"\n",
    "            \"        - Chunk ID: 93833k\\n\"\n",
    "            \"        - Chunk Name: Food Greg likes\\n\"\n",
    "            \"        - Chunk Summary: Lists of the food and dishes that Greg likes\\n\"\n",
    "            \"Output: 93833k\\n\"\n",
    "            f\"Current Chunks:\\n--Start of current chunks--\\n{current_chunk_outline}\\n--End of current chunks--\\n\"\n",
    "            f\"Determine if the following statement should belong to one of the chunks outlined:\\n{proposition}\"\n",
    "            f\"Do not write anything else. Only return the chunk id if you think it should belong to a chunk, or 'No chunks relevant to the proposition' if it should not.\\n\"\n",
    "        )\n",
    "        print(f\"\\n[DEBUG] LLM Prompt:\\n{prompt}\\n\")\n",
    "        chunk_found = self._llm_invoke(prompt).strip()\n",
    "        print(f\"[DEBUG] Chunk Found: {chunk_found}\")\n",
    "        if len(chunk_found) == self.id_truncate_limit and chunk_found in self.chunks:\n",
    "            return chunk_found\n",
    "        return None\n",
    "\n",
    "    def get_chunks(self, get_type: str = 'dict') -> list:\n",
    "        if get_type == 'dict':\n",
    "            return self.chunks\n",
    "        if get_type == 'list_of_strings':\n",
    "            return [\" \".join(chunk['propositions']) for chunk in self.chunks.values()]\n",
    "\n",
    "    def pretty_print_chunks(self):\n",
    "        print(f\"\\nYou have {len(self.chunks)} chunks\\n\")\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            print(f\"Chunk #{chunk['chunk_index']}\")\n",
    "            print(f\"Chunk ID: {chunk_id}\")\n",
    "            print(f\"Summary: {chunk['summary']}\")\n",
    "            print(f\"Propositions:\")\n",
    "            for prop in chunk['propositions']:\n",
    "                print(f\"    - {prop}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "    def pretty_print_chunk_outline(self):\n",
    "        print(\"Chunk Outline\\n\")\n",
    "        print(self.get_chunk_outline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daac6274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'[\"The Quran has content related to various topics.\", '</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Adding: \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"The Quran has content related to various topics.\", '\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No chunks, creating a new one\n",
       "</pre>\n"
      ],
      "text/plain": [
       "No chunks, creating a new one\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Created new chunk <span style=\"font-weight: bold\">(</span>ca87e<span style=\"font-weight: bold\">)</span>: Religious Topics\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Created new chunk \u001b[1m(\u001b[0mca87e\u001b[1m)\u001b[0m: Religious Topics\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'\"The Quran has content related to the Etiquette of Honoring the Guests.\", '</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Adding: \u001b[32m'\"The Quran has content related to the Etiquette of Honoring the Guests.\", '\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>DEBUG<span style=\"font-weight: bold\">]</span> LLM Prompt:\n",
       "Determine whether or not the <span style=\"color: #008000; text-decoration-color: #008000\">'Proposition'</span> should belong to any of the existing chunks.\n",
       "A proposition should belong to a chunk if their meaning, direction, or intention are similar.\n",
       "The goal is to group similar propositions and chunks.\n",
       "If you think a proposition should be joined with a chunk, return the chunk id.\n",
       "If you do not think an item should be joined with an existing chunk, just return <span style=\"color: #008000; text-decoration-color: #008000\">'No chunks'</span>.\n",
       "Example:\n",
       "Input:\n",
       "    - Proposition: <span style=\"color: #008000; text-decoration-color: #008000\">'Greg really likes hamburgers'</span>\n",
       "    - Current Chunks:\n",
       "        - Chunk ID: 2n4l3d\n",
       "        - Chunk Name: Places in San Francisco\n",
       "        - Chunk Summary: Overview of the things to do with San Francisco Places\n",
       "        - Chunk ID: 93833k\n",
       "        - Chunk Name: Food Greg likes\n",
       "        - Chunk Summary: Lists of the food and dishes that Greg likes\n",
       "Output: 93833k\n",
       "Current Chunks:\n",
       "--Start of current chunks--\n",
       "Chunk <span style=\"font-weight: bold\">(</span>ca87e<span style=\"font-weight: bold\">)</span>: Religious Topics\n",
       "Summary: This chunk contains information about the topics covered in religious texts.\n",
       "\n",
       "\n",
       "--End of current chunks--\n",
       "Determine if the following statement should belong to one of the chunks outlined:\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"The Quran has content related to the Etiquette of Honoring the Guests.\"</span>, Do not write anything else. Only return \n",
       "the chunk id if you think it should belong to a chunk, or <span style=\"color: #008000; text-decoration-color: #008000\">'No chunks relevant to the proposition'</span> if it should not.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0mDEBUG\u001b[1m]\u001b[0m LLM Prompt:\n",
       "Determine whether or not the \u001b[32m'Proposition'\u001b[0m should belong to any of the existing chunks.\n",
       "A proposition should belong to a chunk if their meaning, direction, or intention are similar.\n",
       "The goal is to group similar propositions and chunks.\n",
       "If you think a proposition should be joined with a chunk, return the chunk id.\n",
       "If you do not think an item should be joined with an existing chunk, just return \u001b[32m'No chunks'\u001b[0m.\n",
       "Example:\n",
       "Input:\n",
       "    - Proposition: \u001b[32m'Greg really likes hamburgers'\u001b[0m\n",
       "    - Current Chunks:\n",
       "        - Chunk ID: 2n4l3d\n",
       "        - Chunk Name: Places in San Francisco\n",
       "        - Chunk Summary: Overview of the things to do with San Francisco Places\n",
       "        - Chunk ID: 93833k\n",
       "        - Chunk Name: Food Greg likes\n",
       "        - Chunk Summary: Lists of the food and dishes that Greg likes\n",
       "Output: 93833k\n",
       "Current Chunks:\n",
       "--Start of current chunks--\n",
       "Chunk \u001b[1m(\u001b[0mca87e\u001b[1m)\u001b[0m: Religious Topics\n",
       "Summary: This chunk contains information about the topics covered in religious texts.\n",
       "\n",
       "\n",
       "--End of current chunks--\n",
       "Determine if the following statement should belong to one of the chunks outlined:\n",
       "\u001b[32m\"The Quran has content related to the Etiquette of Honoring the Guests.\"\u001b[0m, Do not write anything else. Only return \n",
       "the chunk id if you think it should belong to a chunk, or \u001b[32m'No chunks relevant to the proposition'\u001b[0m if it should not.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>DEBUG<span style=\"font-weight: bold\">]</span> Chunk Found: ca87e\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mDEBUG\u001b[1m]\u001b[0m Chunk Found: ca87e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>ca87e<span style=\"font-weight: bold\">)</span>, adding to: Religious Topics\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chunk Found \u001b[1m(\u001b[0mca87e\u001b[1m)\u001b[0m, adding to: Religious Topics\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'\"The Quran has content related to Riba (Interest).\", '</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Adding: \u001b[32m'\"The Quran has content related to Riba \u001b[0m\u001b[32m(\u001b[0m\u001b[32mInterest\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\", '\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>DEBUG<span style=\"font-weight: bold\">]</span> LLM Prompt:\n",
       "Determine whether or not the <span style=\"color: #008000; text-decoration-color: #008000\">'Proposition'</span> should belong to any of the existing chunks.\n",
       "A proposition should belong to a chunk if their meaning, direction, or intention are similar.\n",
       "The goal is to group similar propositions and chunks.\n",
       "If you think a proposition should be joined with a chunk, return the chunk id.\n",
       "If you do not think an item should be joined with an existing chunk, just return <span style=\"color: #008000; text-decoration-color: #008000\">'No chunks'</span>.\n",
       "Example:\n",
       "Input:\n",
       "    - Proposition: <span style=\"color: #008000; text-decoration-color: #008000\">'Greg really likes hamburgers'</span>\n",
       "    - Current Chunks:\n",
       "        - Chunk ID: 2n4l3d\n",
       "        - Chunk Name: Places in San Francisco\n",
       "        - Chunk Summary: Overview of the things to do with San Francisco Places\n",
       "        - Chunk ID: 93833k\n",
       "        - Chunk Name: Food Greg likes\n",
       "        - Chunk Summary: Lists of the food and dishes that Greg likes\n",
       "Output: 93833k\n",
       "Current Chunks:\n",
       "--Start of current chunks--\n",
       "Chunk <span style=\"font-weight: bold\">(</span>ca87e<span style=\"font-weight: bold\">)</span>: Religious Topics and Content\n",
       "Summary: This chunk contains information about the topics covered in religious texts.\n",
       "\n",
       "\n",
       "--End of current chunks--\n",
       "Determine if the following statement should belong to one of the chunks outlined:\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"The Quran has content related to Riba (Interest).\"</span>, Do not write anything else. Only return the chunk id if you \n",
       "think it should belong to a chunk, or <span style=\"color: #008000; text-decoration-color: #008000\">'No chunks relevant to the proposition'</span> if it should not.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0mDEBUG\u001b[1m]\u001b[0m LLM Prompt:\n",
       "Determine whether or not the \u001b[32m'Proposition'\u001b[0m should belong to any of the existing chunks.\n",
       "A proposition should belong to a chunk if their meaning, direction, or intention are similar.\n",
       "The goal is to group similar propositions and chunks.\n",
       "If you think a proposition should be joined with a chunk, return the chunk id.\n",
       "If you do not think an item should be joined with an existing chunk, just return \u001b[32m'No chunks'\u001b[0m.\n",
       "Example:\n",
       "Input:\n",
       "    - Proposition: \u001b[32m'Greg really likes hamburgers'\u001b[0m\n",
       "    - Current Chunks:\n",
       "        - Chunk ID: 2n4l3d\n",
       "        - Chunk Name: Places in San Francisco\n",
       "        - Chunk Summary: Overview of the things to do with San Francisco Places\n",
       "        - Chunk ID: 93833k\n",
       "        - Chunk Name: Food Greg likes\n",
       "        - Chunk Summary: Lists of the food and dishes that Greg likes\n",
       "Output: 93833k\n",
       "Current Chunks:\n",
       "--Start of current chunks--\n",
       "Chunk \u001b[1m(\u001b[0mca87e\u001b[1m)\u001b[0m: Religious Topics and Content\n",
       "Summary: This chunk contains information about the topics covered in religious texts.\n",
       "\n",
       "\n",
       "--End of current chunks--\n",
       "Determine if the following statement should belong to one of the chunks outlined:\n",
       "\u001b[32m\"The Quran has content related to Riba \u001b[0m\u001b[32m(\u001b[0m\u001b[32mInterest\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\"\u001b[0m, Do not write anything else. Only return the chunk id if you \n",
       "think it should belong to a chunk, or \u001b[32m'No chunks relevant to the proposition'\u001b[0m if it should not.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>DEBUG<span style=\"font-weight: bold\">]</span> Chunk Found: ca87e\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mDEBUG\u001b[1m]\u001b[0m Chunk Found: ca87e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>ca87e<span style=\"font-weight: bold\">)</span>, adding to: Religious Topics and Content\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chunk Found \u001b[1m(\u001b[0mca87e\u001b[1m)\u001b[0m, adding to: Religious Topics and Content\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'\"The Al Quran app allows users to explore topics and read related ayahs.\", '</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Adding: \u001b[32m'\"The Al Quran app allows users to explore topics and read related ayahs.\", '\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>DEBUG<span style=\"font-weight: bold\">]</span> LLM Prompt:\n",
       "Determine whether or not the <span style=\"color: #008000; text-decoration-color: #008000\">'Proposition'</span> should belong to any of the existing chunks.\n",
       "A proposition should belong to a chunk if their meaning, direction, or intention are similar.\n",
       "The goal is to group similar propositions and chunks.\n",
       "If you think a proposition should be joined with a chunk, return the chunk id.\n",
       "If you do not think an item should be joined with an existing chunk, just return <span style=\"color: #008000; text-decoration-color: #008000\">'No chunks'</span>.\n",
       "Example:\n",
       "Input:\n",
       "    - Proposition: <span style=\"color: #008000; text-decoration-color: #008000\">'Greg really likes hamburgers'</span>\n",
       "    - Current Chunks:\n",
       "        - Chunk ID: 2n4l3d\n",
       "        - Chunk Name: Places in San Francisco\n",
       "        - Chunk Summary: Overview of the things to do with San Francisco Places\n",
       "        - Chunk ID: 93833k\n",
       "        - Chunk Name: Food Greg likes\n",
       "        - Chunk Summary: Lists of the food and dishes that Greg likes\n",
       "Output: 93833k\n",
       "Current Chunks:\n",
       "--Start of current chunks--\n",
       "Chunk <span style=\"font-weight: bold\">(</span>ca87e<span style=\"font-weight: bold\">)</span>: Religious Topics and Content\n",
       "Summary: This chunk contains information about the topics covered in religious texts.\n",
       "\n",
       "\n",
       "--End of current chunks--\n",
       "Determine if the following statement should belong to one of the chunks outlined:\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"The Al Quran app allows users to explore topics and read related ayahs.\"</span>, Do not write anything else. Only return \n",
       "the chunk id if you think it should belong to a chunk, or <span style=\"color: #008000; text-decoration-color: #008000\">'No chunks relevant to the proposition'</span> if it should not.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0mDEBUG\u001b[1m]\u001b[0m LLM Prompt:\n",
       "Determine whether or not the \u001b[32m'Proposition'\u001b[0m should belong to any of the existing chunks.\n",
       "A proposition should belong to a chunk if their meaning, direction, or intention are similar.\n",
       "The goal is to group similar propositions and chunks.\n",
       "If you think a proposition should be joined with a chunk, return the chunk id.\n",
       "If you do not think an item should be joined with an existing chunk, just return \u001b[32m'No chunks'\u001b[0m.\n",
       "Example:\n",
       "Input:\n",
       "    - Proposition: \u001b[32m'Greg really likes hamburgers'\u001b[0m\n",
       "    - Current Chunks:\n",
       "        - Chunk ID: 2n4l3d\n",
       "        - Chunk Name: Places in San Francisco\n",
       "        - Chunk Summary: Overview of the things to do with San Francisco Places\n",
       "        - Chunk ID: 93833k\n",
       "        - Chunk Name: Food Greg likes\n",
       "        - Chunk Summary: Lists of the food and dishes that Greg likes\n",
       "Output: 93833k\n",
       "Current Chunks:\n",
       "--Start of current chunks--\n",
       "Chunk \u001b[1m(\u001b[0mca87e\u001b[1m)\u001b[0m: Religious Topics and Content\n",
       "Summary: This chunk contains information about the topics covered in religious texts.\n",
       "\n",
       "\n",
       "--End of current chunks--\n",
       "Determine if the following statement should belong to one of the chunks outlined:\n",
       "\u001b[32m\"The Al Quran app allows users to explore topics and read related ayahs.\"\u001b[0m, Do not write anything else. Only return \n",
       "the chunk id if you think it should belong to a chunk, or \u001b[32m'No chunks relevant to the proposition'\u001b[0m if it should not.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>DEBUG<span style=\"font-weight: bold\">]</span> Chunk Found: ca87e\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mDEBUG\u001b[1m]\u001b[0m Chunk Found: ca87e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>ca87e<span style=\"font-weight: bold\">)</span>, adding to: Religious Topics and Content\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chunk Found \u001b[1m(\u001b[0mca87e\u001b[1m)\u001b[0m, adding to: Religious Topics and Content\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'\"The Al Quran app has 9 important topics broken down into 200+ topics and subtopics.\", '</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Adding: \u001b[32m'\"The Al Quran app has 9 important topics broken down into 200+ topics and subtopics.\", '\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>DEBUG<span style=\"font-weight: bold\">]</span> LLM Prompt:\n",
       "Determine whether or not the <span style=\"color: #008000; text-decoration-color: #008000\">'Proposition'</span> should belong to any of the existing chunks.\n",
       "A proposition should belong to a chunk if their meaning, direction, or intention are similar.\n",
       "The goal is to group similar propositions and chunks.\n",
       "If you think a proposition should be joined with a chunk, return the chunk id.\n",
       "If you do not think an item should be joined with an existing chunk, just return <span style=\"color: #008000; text-decoration-color: #008000\">'No chunks'</span>.\n",
       "Example:\n",
       "Input:\n",
       "    - Proposition: <span style=\"color: #008000; text-decoration-color: #008000\">'Greg really likes hamburgers'</span>\n",
       "    - Current Chunks:\n",
       "        - Chunk ID: 2n4l3d\n",
       "        - Chunk Name: Places in San Francisco\n",
       "        - Chunk Summary: Overview of the things to do with San Francisco Places\n",
       "        - Chunk ID: 93833k\n",
       "        - Chunk Name: Food Greg likes\n",
       "        - Chunk Summary: Lists of the food and dishes that Greg likes\n",
       "Output: 93833k\n",
       "Current Chunks:\n",
       "--Start of current chunks--\n",
       "Chunk <span style=\"font-weight: bold\">(</span>ca87e<span style=\"font-weight: bold\">)</span>: Religious Topics and Content\n",
       "Summary: This chunk contains information about the topics covered in religious texts.\n",
       "\n",
       "\n",
       "--End of current chunks--\n",
       "Determine if the following statement should belong to one of the chunks outlined:\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"The Al Quran app has 9 important topics broken down into 200+ topics and subtopics.\"</span>, Do not write anything else. \n",
       "Only return the chunk id if you think it should belong to a chunk, or <span style=\"color: #008000; text-decoration-color: #008000\">'No chunks relevant to the proposition'</span> if it\n",
       "should not.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0mDEBUG\u001b[1m]\u001b[0m LLM Prompt:\n",
       "Determine whether or not the \u001b[32m'Proposition'\u001b[0m should belong to any of the existing chunks.\n",
       "A proposition should belong to a chunk if their meaning, direction, or intention are similar.\n",
       "The goal is to group similar propositions and chunks.\n",
       "If you think a proposition should be joined with a chunk, return the chunk id.\n",
       "If you do not think an item should be joined with an existing chunk, just return \u001b[32m'No chunks'\u001b[0m.\n",
       "Example:\n",
       "Input:\n",
       "    - Proposition: \u001b[32m'Greg really likes hamburgers'\u001b[0m\n",
       "    - Current Chunks:\n",
       "        - Chunk ID: 2n4l3d\n",
       "        - Chunk Name: Places in San Francisco\n",
       "        - Chunk Summary: Overview of the things to do with San Francisco Places\n",
       "        - Chunk ID: 93833k\n",
       "        - Chunk Name: Food Greg likes\n",
       "        - Chunk Summary: Lists of the food and dishes that Greg likes\n",
       "Output: 93833k\n",
       "Current Chunks:\n",
       "--Start of current chunks--\n",
       "Chunk \u001b[1m(\u001b[0mca87e\u001b[1m)\u001b[0m: Religious Topics and Content\n",
       "Summary: This chunk contains information about the topics covered in religious texts.\n",
       "\n",
       "\n",
       "--End of current chunks--\n",
       "Determine if the following statement should belong to one of the chunks outlined:\n",
       "\u001b[32m\"The Al Quran app has 9 important topics broken down into 200+ topics and subtopics.\"\u001b[0m, Do not write anything else. \n",
       "Only return the chunk id if you think it should belong to a chunk, or \u001b[32m'No chunks relevant to the proposition'\u001b[0m if it\n",
       "should not.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>DEBUG<span style=\"font-weight: bold\">]</span> Chunk Found: ca87e\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mDEBUG\u001b[1m]\u001b[0m Chunk Found: ca87e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>ca87e<span style=\"font-weight: bold\">)</span>, adding to: Religious Topics and Content\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chunk Found \u001b[1m(\u001b[0mca87e\u001b[1m)\u001b[0m, adding to: Religious Topics and Content\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize AgenticChunker and add propositions\n",
    "ac = AgenticChunker()\n",
    "ac.add_propositions(propositions_list[:5])\n",
    "\n",
    "# Get chunks as a dictionary\n",
    "chunks_dict = ac.get_chunks(get_type='dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8afd926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ca87e'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'ca87e'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'propositions'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'[\"The Quran has content related to various topics.\", '</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'\"The Quran has content related to the Etiquette of Honoring the Guests.\", '</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'\"The Quran has content related to Riba (Interest).\", '</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'\"The Al Quran app allows users to explore topics and read related ayahs.\", '</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'\"The Al Quran app has 9 important topics broken down into 200+ topics and subtopics.\", '</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Religious Topics and Content'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'summary'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This chunk contains information about the topics and subtopics covered in religious texts, add </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">propositions about categories, themes, or subjects discussed in sacred writings.'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_index'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'ca87e'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'chunk_id'\u001b[0m: \u001b[32m'ca87e'\u001b[0m,\n",
       "        \u001b[32m'propositions'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "            \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"The Quran has content related to various topics.\", '\u001b[0m,\n",
       "            \u001b[32m'\"The Quran has content related to the Etiquette of Honoring the Guests.\", '\u001b[0m,\n",
       "            \u001b[32m'\"The Quran has content related to Riba \u001b[0m\u001b[32m(\u001b[0m\u001b[32mInterest\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\", '\u001b[0m,\n",
       "            \u001b[32m'\"The Al Quran app allows users to explore topics and read related ayahs.\", '\u001b[0m,\n",
       "            \u001b[32m'\"The Al Quran app has 9 important topics broken down into 200+ topics and subtopics.\", '\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'title'\u001b[0m: \u001b[32m'Religious Topics and Content'\u001b[0m,\n",
       "        \u001b[32m'summary'\u001b[0m: \u001b[32m'This chunk contains information about the topics and subtopics covered in religious texts, add \u001b[0m\n",
       "\u001b[32mpropositions about categories, themes, or subjects discussed in sacred writings.'\u001b[0m,\n",
       "        \u001b[32m'chunk_index'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(chunks_dict))\n",
    "print(chunks_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82330d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">#### Proposition-Based Chunking ####\n",
       "</pre>\n"
      ],
      "text/plain": [
       "#### Proposition-Based Chunking ####\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using device: cuda\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using device: cuda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Agentic Chunking\n",
    "# print(\"#### Proposition-Based Chunking ####\")\n",
    "\n",
    "# # https://arxiv.org/pdf/2312.06648.pdf\n",
    "# import os\n",
    "# from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnableLambda\n",
    "# from langchain.chains import create_extraction_chain\n",
    "# from typing import Optional, List\n",
    "# from langchain.chains import create_extraction_chain_pydantic\n",
    "# from pydantic import BaseModel\n",
    "# from langchain import hub\n",
    "# from langsmith import Client\n",
    "# from cerebras.cloud.sdk import Cerebras\n",
    "# from agentic_chunker import AgenticChunker\n",
    "# import os\n",
    "# import json\n",
    "# import uuid\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import faiss\n",
    "# from dotenv import load_dotenv\n",
    "# from typing import Optional\n",
    "# from rich import print\n",
    "# from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "# from rank_bm25 import BM25Okapi\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.docstore.document import Document\n",
    "# from cerebras.cloud.sdk import Cerebras\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# # Initialize device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "# client = Client(api_key=LANGSMITH_API_KEY)\n",
    "# prompt = client.pull_prompt(\"wfh/proposal-indexing\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Proposition extraction (simplified from Retriver.ipynb)\n",
    "# def extract_propositions(text: str, chunker: AgenticChunker, prompt: ChatPromptTemplate) -> list:\n",
    "#     formatted_prompt = prompt.format(input=text)\n",
    "#     response = chunker._llm_invoke(formatted_prompt)\n",
    "#     propositions = [line.strip() for line in response.split(\"\\n\") if line.strip()]\n",
    "#     return propositions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a32777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Agentic chunking with metadata\n",
    "# def perform_agentic_chunking_with_metadata(documents_json: list, prompt: ChatPromptTemplate) -> list:\n",
    "#     ac = AgenticChunker()\n",
    "#     chunked_docs = []\n",
    "\n",
    "#     for doc in documents_json:\n",
    "#         text = doc.get(\"text\", \"\")\n",
    "#         title = doc.get(\"title\", \"Unknown Title\")\n",
    "#         url = doc.get(\"url\", \"Unknown URL\")\n",
    "\n",
    "#         try:\n",
    "#             propositions = extract_propositions(text, ac, prompt)\n",
    "#             ac.add_propositions(propositions)\n",
    "\n",
    "#             agentic_chunks = ac.get_chunks(get_type=\"list_of_strings\")\n",
    "#             for chunk_text in agentic_chunks:\n",
    "#                 chunked_docs.append(Document(\n",
    "#                     page_content=chunk_text,\n",
    "#                     metadata={\n",
    "#                         \"title\": title,\n",
    "#                         \"url\": url,\n",
    "#                         \"source\": \"agentic\"\n",
    "#                     }\n",
    "#                 ))\n",
    "\n",
    "#             ac = AgenticChunker()  # Reset after each document\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"[Warning] Skipped document: {title}, error: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe89c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"islamic_etiquette_knowledge_base.json\", \"r\") as f1, open(\"Quran_app_Documentation.json\", \"r\") as f2:\n",
    "#     etiquette_data = json.load(f1)\n",
    "#     quran_app_data = json.load(f2)\n",
    "\n",
    "# # combined_documents = etiquette_data + quran_app_data\n",
    "# combined_documents = quran_app_data\n",
    "\n",
    "# # Perform agentic chunking\n",
    "# docs = perform_agentic_chunking_with_metadata(combined_documents, prompt)\n",
    "# print(docs[:2])\n",
    "\n",
    "# # if not docs:\n",
    "# #     raise ValueError(\"No chunked documents found. Please check your chunking process and input data.\")\n",
    "\n",
    "# # # Build indices\n",
    "# # bm25, faiss_index, embeddings, corpus, metadata, chroma = build_index(docs)\n",
    "\n",
    "# # # Example query\n",
    "# # query = \"The Quran app is good but not helpful in understanding the Quranic verses.\"\n",
    "# # prompt = generate_response(query, bm25, faiss_index, corpus, metadata, chroma)\n",
    "# # print(\"\\nGenerated Prompt:\\n\")\n",
    "# # print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Index construction\n",
    "# def build_index(documents: list):\n",
    "#     all_chunks = [doc.page_content for doc in documents]\n",
    "#     metadata = [doc.metadata for doc in documents]\n",
    "\n",
    "#     # Sparse (BM25)\n",
    "#     tokenized_corpus = [chunk.split(\" \") for chunk in all_chunks]\n",
    "#     bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "#     # Dense Embeddings\n",
    "#     embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "#     dense_embeddings = embedding_model.encode(all_chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
    "#     dim = dense_embeddings.shape[1]\n",
    "\n",
    "#     # FAISS (HNSW)\n",
    "#     index = faiss.IndexHNSWFlat(dim, 32)\n",
    "#     index.hnsw.efConstruction = 40\n",
    "#     faiss.normalize_L2(dense_embeddings)\n",
    "#     index.add(dense_embeddings)\n",
    "\n",
    "#     # Chroma\n",
    "#     chroma_db = Chroma.from_texts(\n",
    "#         texts=all_chunks,\n",
    "#         embedding=HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'),\n",
    "#         metadatas=metadata,\n",
    "#         persist_directory=\"./chroma_agentic\"\n",
    "#     )\n",
    "\n",
    "#     return bm25, index, dense_embeddings, all_chunks, metadata, chroma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead36618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hybrid retrieval\n",
    "# # def retrieve_context(query: str, bm25, faiss_index, corpus: list, metadata: list, top_k: int = 50, rerank_k: int = 10) -> tuple:\n",
    "# #     cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device)\n",
    "# #     embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "# #     tokenized_query = query.split(\" \")\n",
    "# #     bm25_scores = bm25.get_scores(tokenized_query)\n",
    "# #     bm25_top_idx = np.argsort(bm25_scores)[::-1][:top_k]\n",
    "\n",
    "# #     query_emb = embedding_model.encode(query, convert_to_numpy=True, normalize_embeddings=True)\n",
    "# #     faiss.normalize_L2(query_emb.reshape(1, -1))\n",
    "# #     _, dense_top_idx = faiss_index.search(query_emb.reshape(1, -1), top_k)\n",
    "\n",
    "# #     candidate_indices = set(bm25_top_idx) | set(dense_top_idx[0])\n",
    "# #     candidates = [(i, corpus[i], metadata[i]) for i in candidate_indices]\n",
    "\n",
    "# #     pairs = [[query, chunk] for _, chunk, _ in candidates]\n",
    "# #     scores = cross_encoder.predict(pairs)\n",
    "# #     reranked = sorted(zip(scores, candidates), key=lambda x: x[0], reverse=True)[:rerank_k]\n",
    "\n",
    "# #     contexts = [chunk for _, (_, chunk, _) in reranked]\n",
    "# #     docs = [meta for _, (_, _, meta) in reranked]\n",
    "# #     return contexts, docs\n",
    "\n",
    "# # Hybrid retrieval with Chroma\n",
    "# def retrieve_context(query: str, bm25, faiss_index, corpus: List[str], metadata: List[dict], chroma_db, top_k: int = 50, rerank_k: int = 15) -> tuple:\n",
    "#     cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device)\n",
    "#     embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "#     # BM25 retrieval\n",
    "#     tokenized_query = query.split(\" \")\n",
    "#     bm25_scores = bm25.get_scores(tokenized_query)\n",
    "#     bm25_top_idx = np.argsort(bm25_scores)[::-1][:top_k]\n",
    "\n",
    "#     # FAISS retrieval\n",
    "#     query_emb = embedding_model.encode(query, convert_to_numpy=True, normalize_embeddings=True)\n",
    "#     faiss.normalize_L2(query_emb.reshape(1, -1))\n",
    "#     _, dense_top_idx = faiss_index.search(query_emb.reshape(1, -1), top_k)\n",
    "\n",
    "#     # Chroma retrieval\n",
    "#     chroma_results = chroma_db.similarity_search_with_score(query, k=top_k)\n",
    "#     chroma_top_idx = [corpus.index(doc.page_content) for doc, _ in chroma_results if doc.page_content in corpus]\n",
    "\n",
    "#     # Combine and deduplicate\n",
    "#     candidate_indices = set(bm25_top_idx) | set(dense_top_idx[0]) | set(chroma_top_idx)\n",
    "#     candidates = [(i, corpus[i], metadata[i]) for i in candidate_indices if i < len(corpus)]\n",
    "\n",
    "#     # Rerank\n",
    "#     pairs = [[query, chunk] for _, chunk, _ in candidates]\n",
    "#     scores = cross_encoder.predict(pairs)\n",
    "#     reranked = sorted(zip(scores, candidates), key=lambda x: x[0], reverse=True)[:rerank_k]\n",
    "\n",
    "#     contexts = [chunk for _, (_, chunk, _) in reranked]\n",
    "#     docs = [meta for _, (_, _, meta) in reranked]\n",
    "#     return contexts, docs\n",
    "\n",
    "# # Generate response (prompt only)\n",
    "# def generate_response(query: str, bm25, faiss_index, corpus: list, metadata: list) -> str:\n",
    "#     contexts, docs = retrieve_context(query, bm25, faiss_index, corpus, metadata)\n",
    "#     combined_context = \"\\n\\n\".join([f\"{doc['source']}:\\n{ctx}\" for ctx, doc in zip(contexts, docs)])\n",
    "#     prompt = f\"Retrieved Chunks:\\n{combined_context}\\n\\nQuery: {query}\"\n",
    "#     return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06850f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, doc in enumerate(docs):\n",
    "#     print(f\"--- Document {i} ---\")\n",
    "#     print(f\"Content: {doc.page_content}\")\n",
    "#     print(f\"Metadata: {doc.metadata}\")\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
