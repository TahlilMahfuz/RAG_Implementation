{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fbc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.chains import create_extraction_chain\n",
    "from typing import Optional, List\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from agentic_chunker import AgenticChunker\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from langsmith import Client\n",
    "import json\n",
    "# from agentic_chunker import AgenticChunker\n",
    "from langchain.docstore.document import Document\n",
    "from dotenv import load_dotenv\n",
    "from rich import print\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community import embeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import torch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc51cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Cerebras client\n",
    "cerebras_api_key = os.getenv(\"CEREBRAS_API_KEY\")\n",
    "if not cerebras_api_key:\n",
    "    raise ValueError(\"CEREBRAS_API_KEY not found in environment variables\")\n",
    "\n",
    "client = Cerebras(api_key=cerebras_api_key)\n",
    "model = \"llama-4-scout-17b-16e-instruct\"\n",
    "\n",
    "# Function to invoke Cerebras API\n",
    "def cerebras_invoke(prompt: str) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=model,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Cerebras API invocation failed: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the proposal-indexing prompt from the hub\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "obj = Client(api_key=LANGSMITH_API_KEY).pull_prompt(\"wfh/proposal-indexing\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cdd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentences(BaseModel):\n",
    "    sentences: List[str]\n",
    "\n",
    "def get_propositions(text, prompt):\n",
    "    formatted_prompt = prompt.format(input=text) + \"\\n\\nOnly provide the list of propositions as output. Do not include any explanations, formatting, or additional text.\"\n",
    "    # print(f\"Formatted Prompt: {formatted_prompt}\")\n",
    "    response = cerebras_invoke(formatted_prompt)\n",
    "    # print(f\"Response: {response}\")\n",
    "    propositions = response.split('\\n')\n",
    "    return {\"proposition\": [Sentences(sentences=propositions)]}, response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "with open(\"islamic_etiquette_knowledge_base.json\", \"r\") as f1, open(\"Quran_app_Documentation.json\", \"r\") as f2:\n",
    "    etiquette_data = json.load(f1)\n",
    "    quran_app_data = json.load(f2)\n",
    "\n",
    "# Use only Quran app data as per the query\n",
    "combined_documents = quran_app_data[:3]\n",
    "\n",
    "# List to hold all proposition arrays with metadata\n",
    "proposition_arrays = []\n",
    "\n",
    "# Process each JSON object\n",
    "for json_obj in combined_documents:\n",
    "    text = json_obj['text']\n",
    "    propositions, response = get_propositions(text, obj)\n",
    "    \n",
    "    # Create an array entry for this document's propositions\n",
    "    document_propositions = {\n",
    "        'metadata': {\n",
    "            'url': json_obj['url'],\n",
    "            'title': json_obj['title']\n",
    "        },\n",
    "        'propositions': [\n",
    "            prop for prop in propositions['proposition'][0].sentences if prop.strip()\n",
    "        ]\n",
    "    }\n",
    "    proposition_arrays.append(document_propositions)\n",
    "\n",
    "# If you need a flat list of all propositions with their metadata:\n",
    "flat_propositions_with_metadata = []\n",
    "for doc in proposition_arrays:\n",
    "    for prop in doc['propositions']:\n",
    "        flat_propositions_with_metadata.append({\n",
    "            'proposition': prop,\n",
    "            'metadata': doc['metadata']\n",
    "        })\n",
    "\n",
    "# And if you just need a simple list of all propositions:\n",
    "propositions_list = [prop for doc in proposition_arrays for prop in doc['propositions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8aed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(propositions_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ddcc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "from typing import Optional\n",
    "from rich import print\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "\n",
    "load_dotenv()\n",
    "class AgenticChunker:\n",
    "    def __init__(self, cerebras_api_key: Optional[str] = None):\n",
    "        self.chunks = {}\n",
    "        self.id_truncate_limit = 5\n",
    "        self.generate_new_metadata_ind = True\n",
    "        self.print_logging = True\n",
    "\n",
    "        if cerebras_api_key is None:\n",
    "            cerebras_api_key = os.getenv(\"CEREBRAS_API_KEY\")\n",
    "        if cerebras_api_key is None:\n",
    "            raise ValueError(\"CEREBRAS_API_KEY not provided or found in environment variables\")\n",
    "\n",
    "        self.client = Cerebras(api_key=cerebras_api_key)\n",
    "        self.model = \"llama-4-scout-17b-16e-instruct\"\n",
    "\n",
    "    def _llm_invoke(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=self.model,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] LLM invocation failed: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def add_propositions(self, propositions: list):\n",
    "        for proposition in propositions:\n",
    "            self.add_proposition(proposition)\n",
    "\n",
    "    def add_proposition(self, proposition: str):\n",
    "        if self.print_logging:\n",
    "            print(f\"\\nAdding: '{proposition}'\")\n",
    "        if len(self.chunks) == 0:\n",
    "            if self.print_logging:\n",
    "                print(\"No chunks, creating a new one\")\n",
    "            self._create_new_chunk(proposition)\n",
    "            return\n",
    "\n",
    "        chunk_id = self._find_relevant_chunk(proposition)\n",
    "        if chunk_id:\n",
    "            if self.print_logging:\n",
    "                print(f\"Chunk Found ({self.chunks[chunk_id]['chunk_id']}), adding to: {self.chunks[chunk_id]['title']}\")\n",
    "            self.add_proposition_to_chunk(chunk_id, proposition)\n",
    "        else:\n",
    "            if self.print_logging:\n",
    "                print(\"No chunks found\")\n",
    "            self._create_new_chunk(proposition)\n",
    "\n",
    "    def add_proposition_to_chunk(self, chunk_id: str, proposition: str):\n",
    "        self.chunks[chunk_id]['propositions'].append(proposition)\n",
    "        if self.generate_new_metadata_ind:\n",
    "            self.chunks[chunk_id]['summary'] = self._update_chunk_summary(self.chunks[chunk_id])\n",
    "            self.chunks[chunk_id]['title'] = self._update_chunk_title(self.chunks[chunk_id])\n",
    "\n",
    "    def _update_chunk_summary(self, chunk: dict) -> str:\n",
    "        prompt = (\n",
    "            \"You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic.\\n\"\n",
    "            \"A new proposition was just added to one of your chunks. Generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\\n\"\n",
    "            \"A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\\n\"\n",
    "            \"Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food. Or month, generalize it to 'date and times'.\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"Input: Proposition: Greg likes to eat pizza\\n\"\n",
    "            \"Output: This chunk contains information about the types of food Greg likes to eat.\\n\"\n",
    "            \"Only respond with the chunk new summary, nothing else.\\n\"\n",
    "            f\"Chunk's propositions:\\n\" + \"\\n\".join(chunk['propositions']) +\n",
    "            f\"\\n\\nCurrent chunk summary:\\n{chunk['summary']}\"\n",
    "        )\n",
    "        return self._llm_invoke(prompt)\n",
    "\n",
    "    def _update_chunk_title(self, chunk: dict) -> str:\n",
    "        prompt = (\n",
    "            \"You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic.\\n\"\n",
    "            \"A new proposition was just added to one of your chunks, you should generate a very brief updated chunk title which will inform viewers what a chunk group is about.\\n\"\n",
    "            \"A good title will say what the chunk is about.\\n\"\n",
    "            \"You will be given a group of propositions which are in the chunk, chunk summary and the chunk title.\\n\"\n",
    "            \"Your title should anticipate generalization. If you get a proposition about apples, generalize it to food. Or month, generalize it to \\\"date and times\\\".\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"Input: Summary: This chunk is about dates and times that the author talks about\\n\"\n",
    "            \"Output: Date & Times\\n\"\n",
    "            \"Only respond with the new chunk title, nothing else.\\n\"\n",
    "            f\"Chunk's propositions:\\n\" + \"\\n\".join(chunk['propositions']) +\n",
    "            f\"\\n\\nChunk summary:\\n{chunk['summary']}\\n\\nCurrent chunk title:\\n{chunk['title']}\"\n",
    "        )\n",
    "        return self._llm_invoke(prompt)\n",
    "\n",
    "    def _get_new_chunk_summary(self, proposition: str) -> str:\n",
    "        prompt = (\n",
    "            \"You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic.\\n\"\n",
    "            \"You should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\\n\"\n",
    "            \"A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\\n\"\n",
    "            \"You will be given a proposition which will go into a new chunk. This new chunk needs a summary.\\n\"\n",
    "            \"Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food. Or month, generalize it to \\\"date and times\\\".\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"Input: Proposition: Greg likes to eat pizza\\n\"\n",
    "            \"Output: This chunk contains information about the types of food Greg likes to eat.\\n\"\n",
    "            \"Only respond with the new chunk summary, nothing else.\\n\"\n",
    "            f\"Determine the summary of the new chunk that this proposition will go into:\\n{proposition}\"\n",
    "        )\n",
    "        return self._llm_invoke(prompt)\n",
    "\n",
    "    def _get_new_chunk_title(self, summary: str) -> str:\n",
    "        prompt = (\n",
    "            \"You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic.\\n\"\n",
    "            \"You should generate a very brief few word chunk title which will inform viewers what a chunk group is about.\\n\"\n",
    "            \"A good chunk title is brief but encompasses what the chunk is about.\\n\"\n",
    "            \"You will be given a summary of a chunk which needs a title.\\n\"\n",
    "            \"Your titles should anticipate generalization. If you get a proposition about apples, generalize it to food. Or month, generalize it to \\\"date and times\\\".\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"Input: Summary: This chunk is about dates and times that the author talks about\\n\"\n",
    "            \"Output: Date & Times\\n\"\n",
    "            \"Only respond with the new chunk title, nothing else.\\n\"\n",
    "            f\"Determine the title of the chunk that this summary belongs to:\\n{summary}\"\n",
    "        )\n",
    "        return self._llm_invoke(prompt)\n",
    "\n",
    "    def _create_new_chunk(self, proposition: str):\n",
    "        new_chunk_id = str(uuid.uuid4())[:self.id_truncate_limit]\n",
    "        new_chunk_summary = self._get_new_chunk_summary(proposition)\n",
    "        new_chunk_title = self._get_new_chunk_title(new_chunk_summary)\n",
    "        self.chunks[new_chunk_id] = {\n",
    "            'chunk_id': new_chunk_id,\n",
    "            'propositions': [proposition],\n",
    "            'title': new_chunk_title,\n",
    "            'summary': new_chunk_summary,\n",
    "            'chunk_index': len(self.chunks)\n",
    "        }\n",
    "        if self.print_logging:\n",
    "            print(f\"Created new chunk ({new_chunk_id}): {new_chunk_title}\")\n",
    "\n",
    "    def get_chunk_outline(self) -> str:\n",
    "        chunk_outline = \"\"\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            single_chunk_string = f\"\"\"Chunk ({chunk['chunk_id']}): {chunk['title']}\\nSummary: {chunk['summary']}\\n\\n\"\"\"\n",
    "            chunk_outline += single_chunk_string\n",
    "        return chunk_outline\n",
    "\n",
    "    def _find_relevant_chunk(self, proposition: str) -> Optional[str]:\n",
    "        current_chunk_outline = self.get_chunk_outline()\n",
    "        prompt = (\n",
    "            \"Determine whether or not the 'Proposition' should belong to any of the existing chunks.\\n\"\n",
    "            \"A proposition should belong to a chunk if their meaning, direction, or intention are similar.\\n\"\n",
    "            \"The goal is to group similar propositions and chunks.\\n\"\n",
    "            \"If you think a proposition should be joined with a chunk, return the chunk id.\\n\"\n",
    "            \"If you do not think an item should be joined with an existing chunk, just return 'No chunks'.\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"Input:\\n\"\n",
    "            \"    - Proposition: 'Greg really likes hamburgers'\\n\"\n",
    "            \"    - Current Chunks:\\n\"\n",
    "            \"        - Chunk ID: 2n4l3d\\n\"\n",
    "            \"        - Chunk Name: Places in San Francisco\\n\"\n",
    "            \"        - Chunk Summary: Overview of the things to do with San Francisco Places\\n\"\n",
    "            \"        - Chunk ID: 93833k\\n\"\n",
    "            \"        - Chunk Name: Food Greg likes\\n\"\n",
    "            \"        - Chunk Summary: Lists of the food and dishes that Greg likes\\n\"\n",
    "            \"Output: 93833k\\n\"\n",
    "            f\"Current Chunks:\\n--Start of current chunks--\\n{current_chunk_outline}\\n--End of current chunks--\\n\"\n",
    "            f\"Determine if the following statement should belong to one of the chunks outlined:\\n{proposition}\"\n",
    "            f\"Do not write anything else. Only return the chunk id if you think it should belong to a chunk, or 'No chunks relevant to the proposition' if it should not.\\n\"\n",
    "        )\n",
    "        print(f\"\\n[DEBUG] LLM Prompt:\\n{prompt}\\n\")\n",
    "        chunk_found = self._llm_invoke(prompt).strip()\n",
    "        print(f\"[DEBUG] Chunk Found: {chunk_found}\")\n",
    "        if len(chunk_found) == self.id_truncate_limit and chunk_found in self.chunks:\n",
    "            return chunk_found\n",
    "        return None\n",
    "\n",
    "    def get_chunks(self, get_type: str = 'dict') -> list:\n",
    "        if get_type == 'dict':\n",
    "            return self.chunks\n",
    "        if get_type == 'list_of_strings':\n",
    "            return [\" \".join(chunk['propositions']) for chunk in self.chunks.values()]\n",
    "\n",
    "    def pretty_print_chunks(self):\n",
    "        print(f\"\\nYou have {len(self.chunks)} chunks\\n\")\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            print(f\"Chunk #{chunk['chunk_index']}\")\n",
    "            print(f\"Chunk ID: {chunk_id}\")\n",
    "            print(f\"Summary: {chunk['summary']}\")\n",
    "            print(f\"Propositions:\")\n",
    "            for prop in chunk['propositions']:\n",
    "                print(f\"    - {prop}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "    def pretty_print_chunk_outline(self):\n",
    "        print(\"Chunk Outline\\n\")\n",
    "        print(self.get_chunk_outline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AgenticChunker and add propositions\n",
    "ac = AgenticChunker()\n",
    "ac.add_propositions(propositions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68714a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chunks as a dictionary\n",
    "chunks_dict = ac.get_chunks(get_type='dict')\n",
    "print(len(chunks_dict))\n",
    "print(chunks_dict)\n",
    "# print(ac.pretty_print_chunks())\n",
    "# chunks = ac.get_chunks(get_type='list_of_strings')\n",
    "# print(chunks)\n",
    "# documents = [Document(page_content=chunk, metadata={\"source\": \"local\"}) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29da045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Create a list of Document objects from chunk_dict for embedding\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=re.sub(r'^[\\[\\]\"\\',\\s]+|[\\[\\]\"\\',\\s]+$', '', \" \".join(chunk['propositions'])),\n",
    "        metadata=chunk['metadata'] if 'metadata' in chunk else {\n",
    "            \"chunk_id\": chunk['chunk_id'],\n",
    "            \"title\": chunk['title'],\n",
    "            \"summary\": chunk['summary'],\n",
    "            \"chunk_index\": chunk['chunk_index']\n",
    "        }\n",
    "    )\n",
    "    for chunk in chunks_dict.values()\n",
    "]\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b8368",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index construction\n",
    "def build_index(documents: list):\n",
    "    all_chunks = [doc.page_content for doc in documents]\n",
    "    metadata = [doc.metadata for doc in documents]\n",
    "\n",
    "    # Sparse (BM25)\n",
    "    tokenized_corpus = [chunk.split(\" \") for chunk in all_chunks]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    # Dense Embeddings\n",
    "    embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "    dense_embeddings = embedding_model.encode(all_chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    dim = dense_embeddings.shape[1]\n",
    "\n",
    "    # FAISS (HNSW)\n",
    "    index = faiss.IndexHNSWFlat(dim, 32)\n",
    "    index.hnsw.efConstruction = 40\n",
    "    faiss.normalize_L2(dense_embeddings)\n",
    "    index.add(dense_embeddings)\n",
    "\n",
    "    # Chroma\n",
    "    chroma_db = Chroma.from_texts(\n",
    "        texts=all_chunks,\n",
    "        embedding=HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'),\n",
    "        metadatas=metadata,\n",
    "        persist_directory=\"./chroma_agentic\"\n",
    "    )\n",
    "\n",
    "    return bm25, index, dense_embeddings, all_chunks, metadata, chroma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead36618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid retrieval with Chroma\n",
    "def retrieve_context(query: str, bm25, faiss_index, corpus: List[str], metadata: List[dict], chroma_db, top_k: int = 50, rerank_k: int = 15) -> tuple:\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device)\n",
    "    embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "    # BM25 retrieval\n",
    "    tokenized_query = query.split(\" \")\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    bm25_top_idx = np.argsort(bm25_scores)[::-1][:top_k]\n",
    "\n",
    "    # FAISS retrieval\n",
    "    query_emb = embedding_model.encode(query, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    faiss.normalize_L2(query_emb.reshape(1, -1))\n",
    "    _, dense_top_idx = faiss_index.search(query_emb.reshape(1, -1), top_k)\n",
    "\n",
    "    # Chroma retrieval\n",
    "    chroma_results = chroma_db.similarity_search_with_score(query, k=top_k)\n",
    "    chroma_top_idx = [corpus.index(doc.page_content) for doc, _ in chroma_results if doc.page_content in corpus]\n",
    "\n",
    "    # Combine and deduplicate\n",
    "    candidate_indices = set(bm25_top_idx) | set(dense_top_idx[0]) | set(chroma_top_idx)\n",
    "    candidates = [(i, corpus[i], metadata[i]) for i in candidate_indices if i < len(corpus)]\n",
    "\n",
    "    # Rerank\n",
    "    pairs = [[query, chunk] for _, chunk, _ in candidates]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    reranked = sorted(zip(scores, candidates), key=lambda x: x[0], reverse=True)[:rerank_k]\n",
    "\n",
    "    contexts = [chunk for _, (_, chunk, _) in reranked]\n",
    "    docs = [meta for _, (_, _, meta) in reranked]\n",
    "\n",
    "    # print all the results of the retrieval techniques\n",
    "    print(f\"BM25 Top Indices: {bm25_top_idx}\")\n",
    "    print(f\"FAISS Top Indices: {dense_top_idx[0]}\")\n",
    "    print(f\"Chroma Top Indices: {chroma_top_idx}\")\n",
    "    print(f\"Combined Candidate Indices: {candidate_indices}\")\n",
    "    print(f\"Reranked Contexts: {contexts}\")\n",
    "    return contexts, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str, bm25, faiss_index, corpus: list, metadata: list, chroma_db) -> str:\n",
    "    contexts, docs = retrieve_context(query, bm25, faiss_index, corpus, metadata, chroma_db)\n",
    "    combined_context = \"\\n\\n\".join([f\"{doc.get('source', doc.get('title', ''))}:\\n{ctx}\" for ctx, doc in zip(contexts, docs)])\n",
    "    prompt = f\"Retrieved Chunks:\\n{combined_context}\\n\\nQuery: {query}\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5faa20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save documents\n",
    "with open(\"documents.json\", \"w\") as f:\n",
    "    json.dump([{\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in documents], f)\n",
    "\n",
    "# Build index\n",
    "bm25, index, dense_embeddings, all_chunks, metadata, chroma_db = build_index(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe89c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generation\n",
    "review_text = \"Quran app audio is not working properly. I can't understand how to use the audio feature clearly. Also searching is not working\"\n",
    "response = generate_response(review_text, bm25, index, all_chunks, metadata, chroma_db)\n",
    "\n",
    "# print with wraptext\n",
    "print(\"Generated Response:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
