{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tahlilmahfuz/thesis/RAG_Implementation/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dpr_question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "dpr_context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoder models\n",
    "dpr_context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-encoder for reranking\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device)\n",
    "\n",
    "# Generator model\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 200, overlap: int = 50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunks.append(' '.join(words[i:i+chunk_size]))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('islamic_etiquette_knowledge_base.json') as f:\n",
    "    etiquette_data = json.load(f)\n",
    "\n",
    "raw_entries = etiquette_data\n",
    "# Create passages\n",
    "passages = []\n",
    "metadata = []\n",
    "for entry in raw_entries:\n",
    "    for chunk in chunk_text(entry['text']):\n",
    "        passages.append(chunk)\n",
    "        metadata.append({'url': entry['url'], 'title': entry['title'], 'text': chunk})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 index\n",
    "tokenized_corpus = [p.split() for p in passages]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Build DPR (FAISS) index\n",
    "ctx_embeddings = []\n",
    "for txt in passages:\n",
    "    inputs = dpr_context_tokenizer(txt, return_tensors='pt', truncation=True, max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = dpr_context_encoder(**inputs).pooler_output\n",
    "    ctx_embeddings.append(emb.cpu().numpy())\n",
    "ctx_embeddings = np.vstack(ctx_embeddings)\n",
    "index = faiss.IndexFlatIP(ctx_embeddings.shape[1])\n",
    "faiss.normalize_L2(ctx_embeddings)\n",
    "index.add(ctx_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval function\n",
    "def retrieve_relevant_context(query: str, top_k: int = 5):\n",
    "    # BM25 retrieval\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    bm25_top = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k]\n",
    "\n",
    "    # DPR retrieval\n",
    "    q_inputs = dpr_question_tokenizer(query, return_tensors='pt', truncation=True, max_length=64).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_emb = dpr_question_encoder(**q_inputs).pooler_output.cpu().numpy()\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    dpr_top = I[0]\n",
    "\n",
    "    # Combine BM25 and DPR\n",
    "    combined = set(bm25_top) | set(dpr_top)\n",
    "    candidates = list(combined)\n",
    "    # Rerank with cross-encoder\n",
    "    cross_in = [(query, passages[idx]) for idx in candidates]\n",
    "    cross_scores = cross_encoder.predict(cross_in)\n",
    "    reranked = [candidates[i] for i in sorted(range(len(cross_scores)), key=lambda i: cross_scores[i], reverse=True)][:top_k]\n",
    "\n",
    "    return [passages[i] for i in reranked], [metadata[i] for i in reranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response generation\n",
    "def generate_response(review: str) -> str:\n",
    "    contexts, docs = retrieve_relevant_context(review, top_k=3)\n",
    "    combined_context = \"\\n\\n\".join([f\"[{doc['title']}]({doc['url']}): {ctx}\" for ctx, doc in zip(contexts, docs)])\n",
    "    prompt = f\"Context:\\n{combined_context}\\n\\nReview: {review}\\n\\nResponse:\"\n",
    "\n",
    "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    outputs = gen_model.generate(**inputs, max_new_tokens=150)\n",
    "    return gen_tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = \"The app is useful but lacks proper notifications for prayer times.\"\n",
    "print(generate_response(review_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
